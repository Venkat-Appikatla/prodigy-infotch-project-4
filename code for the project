import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
import joblib

# Define directories
data_dir = 'path_to_hand_gesture_dataset'
gestures = ['gesture1', 'gesture2', 'gesture3']  # Replace with actual gesture names

# Image preprocessing parameters
img_size = (64, 64)  # Resize images to 64x64

# Initialize arrays to hold image data and labels
data = []
labels = []

# Load images
def load_images_from_folder(folder, label):
    images = os.listdir(folder)
    for img_name in images:
        img_path = os.path.join(folder, img_name)
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        img = cv2.resize(img, img_size)
        img_flattened = img.flatten()
        data.append(img_flattened)
        labels.append(label)

# Load images for each gesture
for gesture in gestures:
    gesture_folder = os.path.join(data_dir, gesture)
    load_images_from_folder(gesture_folder, gesture)

# Convert data and labels to numpy arrays
data = np.array(data)
labels = np.array(labels)

# Encode labels
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)

# Initialize the SVM classifier
svm = SVC(kernel='linear', C=1.0, random_state=42)

# Train the classifier
svm.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svm.predict(X_test)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Print classification report
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# Save the trained model to a file
joblib.dump(svm, 'svm_hand_gesture_classifier.pkl')

# Function to preprocess and predict a new image
def preprocess_and_predict(img_path):
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    img = cv2.resize(img, img_size)
    img_flattened = img.flatten()
    img_flattened = img_flattened.reshape(1, -1)
    prediction = svm.predict(img_flattened)
    return label_encoder.inverse_transform(prediction)[0]

# Example usage
img_path = 'path_to_new_image.jpg'
result = preprocess_and_predict(img_path)
print(f"The gesture is predicted to be: {result}")

# Real-time gesture recognition
def real_time_gesture_recognition():
    cap = cv2.VideoCapture(0)  # Use 0 for webcam, or provide video file path

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        gray = cv2.resize(gray, img_size)
        img_flattened = gray.flatten().reshape(1, -1)

        prediction = svm.predict(img_flattened)
        gesture = label_encoder.inverse_transform(prediction)[0]

        cv2.putText(frame, f'Gesture: {gesture}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
        cv2.imshow('Gesture Recognition', frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

# Uncomment the line below to run real-time gesture recognition
# real_time_gesture_recognition()
